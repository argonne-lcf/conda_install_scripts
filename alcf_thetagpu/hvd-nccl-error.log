2021-06-28 18:59:26.872675: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-28 18:59:26.872777: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-06-28 18:59:27.457469: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38427 MB memory:  -> device: 1, name: A100-SXM4-40GB, pci bus id: 0000:0f:00.0, compute capability: 8.0
2021-06-28 18:59:27.460242: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1510] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38427 MB memory:  -> device: 0, name: A100-SXM4-40GB, pci bus id: 0000:07:00.0, compute capability: 8.0
2021-06-28 18:59:29.719528: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-28 18:59:30.353599: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:185] None of the MLIR Optimization Passes are enabled (registered 2)
2021-06-28 18:59:30.438894: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200
2021-06-28 18:59:31.241442: I tensorflow/stream_executor/cuda/cuda_dnn.cc:369] Loaded cuDNN version 8200
2021-06-28 18:59:31.940518: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
2021-06-28 18:59:32.940915: I tensorflow/stream_executor/cuda/cuda_blas.cc:1760] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
thetagpu16:334901:334941 [0] NCCL INFO Bootstrap : Using enp226s0:10.230.2.204<0>
thetagpu16:334901:334941 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
thetagpu16:334901:334941 [0] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE ; OOB enp226s0:10.230.2.204<0>
thetagpu16:334901:334941 [0] NCCL INFO Using network IB
NCCL version 2.9.8+cuda11.3
thetagpu16:334902:334940 [1] NCCL INFO Bootstrap : Using enp226s0:10.230.2.204<0>
thetagpu16:334902:334940 [1] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
thetagpu16:334902:334940 [1] NCCL INFO NET/IB : Using [0]mlx5_0:1/IB [1]mlx5_1:1/IB [2]mlx5_2:1/IB [3]mlx5_3:1/IB [4]mlx5_4:1/IB [5]mlx5_5:1/RoCE [6]mlx5_6:1/IB [7]mlx5_7:1/IB [8]mlx5_8:1/IB [9]mlx5_9:1/IB [10]mlx5_10:1/IB [11]mlx5_11:1/RoCE ; OOB enp226s0:10.230.2.204<0>
thetagpu16:334902:334940 [1] NCCL INFO Using network IB
thetagpu16:334901:334941 [0] NCCL INFO Channel 00/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 01/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 02/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 03/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 04/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 05/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 06/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 07/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 08/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 09/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 10/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 11/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 12/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 13/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 14/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 15/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 16/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 17/24 :    0   1
thetagpu16:334902:334940 [1] NCCL INFO Trees [0] -1/-1/-1->1->0 [1] -1/-1/-1->1->0 [2] -1/-1/-1->1->0 [3] -1/-1/-1->1->0 [4] -1/-1/-1->1->0 [5] -1/-1/-1->1->0 [6] -1/-1/-1->1->0 [7] -1/-1/-1->1->0 [8] -1/-1/-1->1->0 [9] -1/-1/-1->1->0 [10] -1/-1/-1->1->0 [11] -1/-1/-1->1->0 [12] -1/-1/-1->1->0 [13] -1/-1/-1->1->0 [14] -1/-1/-1->1->0 [15] -1/-1/-1->1->0 [16] -1/-1/-1->1->0 [17] -1/-1/-1->1->0 [18] -1/-1/-1->1->0 [19] -1/-1/-1->1->0 [20] -1/-1/-1->1->0 [21] -1/-1/-1->1->0 [22] -1/-1/-1->1->0 [23] -1/-1/-1->1->0
thetagpu16:334901:334941 [0] NCCL INFO Channel 18/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 19/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 20/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 21/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 22/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Channel 23/24 :    0   1
thetagpu16:334901:334941 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
thetagpu16:334901:334941 [0] NCCL INFO Channel 00 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 01 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 02 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 03 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 04 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 05 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 06 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 07 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 08 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 09 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 10 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 11 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 12 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 13 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 14 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 15 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 16 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 17 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 18 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 19 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 20 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 00 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 21 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 01 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 22 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 02 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Channel 23 : 0[7000] -> 1[f000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 03 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 04 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 05 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 06 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 07 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 08 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 09 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 10 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 11 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 12 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 13 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 14 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 15 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 16 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 17 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 18 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 19 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 20 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 21 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 22 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334902:334940 [1] NCCL INFO Channel 23 : 1[f000] -> 0[7000] via P2P/IPC/read
thetagpu16:334901:334941 [0] NCCL INFO Connected all rings
thetagpu16:334901:334941 [0] NCCL INFO Connected all trees
thetagpu16:334901:334941 [0] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
thetagpu16:334902:334940 [1] NCCL INFO Connected all rings
thetagpu16:334902:334940 [1] NCCL INFO Connected all trees
thetagpu16:334902:334940 [1] NCCL INFO threadThresholds 8/8/64 | 16/8/64 | 8/8/512
thetagpu16:334901:334941 [0] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
thetagpu16:334902:334940 [1] NCCL INFO 24 coll channels, 32 p2p channels, 32 p2p channels per peer
thetagpu16:334901:334941 [0] NCCL INFO comm 0x7f57fd6b79e0 rank 0 nranks 2 cudaDev 0 busId 7000 - Init COMPLETE
thetagpu16:334902:334940 [1] NCCL INFO comm 0x7f0c616b70b0 rank 1 nranks 2 cudaDev 1 busId f000 - Init COMPLETE

thetagpu16:334901:334941 [0] enqueue.cc:802 NCCL WARN Cuda failure 'API call is not supported in the installed CUDA driver'
thetagpu16:334901:334941 [0] NCCL INFO enqueue.cc:884 -> 1

thetagpu16:334902:334940 [1] enqueue.cc:802 NCCL WARN Cuda failure 'API call is not supported in the installed CUDA driver'
thetagpu16:334902:334940 [1] NCCL INFO enqueue.cc:884 -> 1

thetagpu16:334901:334941 [0] misc/argcheck.cc:39 NCCL WARN AllReduce : invalid root 0 (root should be in the 0..-1 range)
thetagpu16:334901:334941 [0] NCCL INFO enqueue.cc:874 -> 4

thetagpu16:334901:334941 [0] init.cc:896 NCCL WARN Cuda failure 'invalid device ordinal'

thetagpu16:334902:334940 [1] misc/argcheck.cc:39 NCCL WARN AllReduce : invalid root 0 (root should be in the 0..-1 range)
thetagpu16:334902:334940 [1] NCCL INFO enqueue.cc:874 -> 4

thetagpu16:334902:334940 [1] init.cc:896 NCCL WARN Cuda failure 'invalid device ordinal'
Traceback (most recent call last):
  File "./tf2_hvd_mnist.py", line 216, in <module>
    main(args)
  File "./tf2_hvd_mnist.py", line 166, in main
    train(epoch, data['training'], model, loss_fn, optimizer, args, metrics)
  File "./tf2_hvd_mnist.py", line 120, in train
    loss, output = train_step(data, model, loss_fn, optimizer,
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 3039, in __call__
    return graph_function._call_flat(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 591, in call
    outputs = execute.execute(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError:  ncclAllReduce failed: unhandled cuda error
	 [[{{node DistributedGradientTape_Allreduce/cond_6/then/_48/DistributedGradientTape_Allreduce/cond_6/HorovodAllreduce_gradient_tape_sequential_dense_1_MatMul_1_0}}]] [Op:__inference_train_step_996]

Function call stack:
train_step

Traceback (most recent call last):
  File "./tf2_hvd_mnist.py", line 216, in <module>
    main(args)
  File "./tf2_hvd_mnist.py", line 166, in main
    train(epoch, data['training'], model, loss_fn, optimizer, args, metrics)
  File "./tf2_hvd_mnist.py", line 120, in train
    loss, output = train_step(data, model, loss_fn, optimizer,
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 885, in __call__
    result = self._call(*args, **kwds)
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py", line 950, in _call
    return self._stateless_fn(*args, **kwds)
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 3039, in __call__
    return graph_function._call_flat(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 1963, in _call_flat
    return self._build_call_outputs(self._inference_function.call(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/function.py", line 591, in call
    outputs = execute.execute(
  File "/lus/theta-fs0/software/thetagpu/conda/deephyper/latest/mconda3/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.UnknownError:  ncclAllReduce failed: unhandled cuda error
	 [[{{node DistributedGradientTape_Allreduce/cond_6/then/_48/DistributedGradientTape_Allreduce/cond_6/HorovodAllreduce_gradient_tape_sequential_dense_1_MatMul_1_0}}]] [Op:__inference_train_step_996]

Function call stack:
train_step


thetagpu16:334902:334940 [1] init.cc:923 NCCL WARN comm 0x7f0c616b70b0 has already been destroyed

thetagpu16:334901:334941 [0] init.cc:923 NCCL WARN comm 0x7f57fd6b79e0 has already been destroyed
--------------------------------------------------------------------------
Primary job  terminated normally, but 1 process returned
a non-zero exit code. Per user-direction, the job has been aborted.
--------------------------------------------------------------------------
--------------------------------------------------------------------------
orterun detected that one or more processes exited with non-zero status, thus causing
the job to be terminated. The first process to do so was:

  Process name: [[27461,1],0]
  Exit code:    1
--------------------------------------------------------------------------
